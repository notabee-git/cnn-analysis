



# -*- coding: utf-8 -*-
"""DLFA_CNN_A4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/dlfa-cnn-a4-8921452a-9948-42e3-ae88-283ffd60b366.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240401/auto/storage/goog4_request%26X-Goog-Date%3D20240401T113021Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3308ca11b689882c1e879c50c74cc64ab4f39eb2956494c7f3f6abeed075d5df8fe45cb1e5f8d8ed3f640075fe3b8dcabb1e15b96f4eac62211339a32a516bcfdfa8c68da3f779e0cb8e436b647bd864d827cda05bbcb3e9d84be4e6a1977be238d0e0b2533706f7965d0a6653e2aa37d2bc893513f58682c2ba7d1f480fe901f5017845cadcf2ce43a9e8f0c70605ff7f4a5836ed43bf84e41343a07039318ef49dc07ce062990c690c0ff46a6fd82bb094d5e2969bfdd3c420dff504ce308da580cf884118a087ef850b179dd6c03c435cb4eb6ee17dc5617c743ff68bea94e568835182198f2848410a5d8f166cc49c6daa1451f4abb96304e5145141a7ce
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Import necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision.datasets import CIFAR10

"""# Implementation of CNN Vanilla and CNN Resnet

### Defining CNN-Vanilla
"""

import torch  # Import PyTorch library
import torch.nn as nn  # Import nn module for defining neural network layers

class VanillaBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(VanillaBlock, self).__init__()
        # Define first convolutional layer with batch normalization and ReLU activation
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)  # Batch normalization
        self.relu = nn.ReLU(inplace=True)  # ReLU activation
        # Define second convolutional layer with batch normalization and ReLU activation
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)  # Batch normalization
        self.stride = stride  # Store stride value for later use

    def forward(self, x):
        # Forward pass through the block
        out = self.relu(self.bn1(self.conv1(x)))  # Apply first convolution, batch normalization, and ReLU
        out = self.bn2(self.conv2(out))  # Apply second convolution and batch normalization
        out = self.relu(out)  # Apply ReLU
        return out  # Return output

class CNNVanilla(nn.Module):
    def __init__(self):
        super(CNNVanilla, self).__init__()
        # Define initial convolutional layer
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer
        # Define three VanillaBlocks
        self.vanilla_block1 = VanillaBlock(32, 64)
        self.vanilla_block2 = VanillaBlock(64, 64)
        self.vanilla_block3 = VanillaBlock(64, 64)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # Fully connected layer
        self.fc2 = nn.Linear(128, 10)  # Output layer

    def forward(self, x):
        # Forward pass through the network
        x = self.pool(torch.relu(self.conv1(x)))  # Apply initial convolution, ReLU, and pooling
        x = self.vanilla_block1(x)  # Pass through first VanillaBlock
        x = self.vanilla_block2(x)  # Pass through second VanillaBlock
        x = self.vanilla_block3(x)  # Pass through third VanillaBlock
        x = self.pool(x)  # Pooling
        x = x.view(-1, 64 * 8 * 8)  # Reshape tensor - Flattening
        x = torch.relu(self.fc1(x))  # Apply ReLU to first fully connected layer
        x = self.fc2(x)  # Apply second fully connected layer
        return x  # Return output

"""### Defining CNN-Resnet"""

# Define the ResNet block
class ResNetBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResNetBlock, self).__init__()
        # Define first convolutional layer with batch normalization and ReLU activation
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)  # Batch normalization
        self.relu = nn.ReLU(inplace=True)  # ReLU activation
        # Define second convolutional layer with batch normalization
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)  # Batch normalization
        self.stride = stride  # Store stride value for later use

        # Define shortcut connection
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Sequential()  # Empty sequential module

    def forward(self, x):
        residual = x  # Store input as residual
        out = self.relu(self.bn1(self.conv1(x)))  # Apply first convolution, batch normalization, and ReLU
        out = self.bn2(self.conv2(out))  # Apply second convolution and batch normalization
        out += self.shortcut(residual)  # Add shortcut connection
        out = self.relu(out)  # Apply ReLU
        return out  # Return output

class CNNResNet(nn.Module):
    def __init__(self):
        super(CNNResNet, self).__init__()
        # Define initial convolutional layer
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer
        # Define three ResNetBlocks
        self.resnet_block1 = ResNetBlock(32, 64)
        self.resnet_block2 = ResNetBlock(64, 64)
        self.resnet_block3 = ResNetBlock(64, 64)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # Fully connected layer
        self.fc2 = nn.Linear(128, 10)  # Output layer

    def forward(self, x):
        # Forward pass through the network
        x = self.pool(torch.relu(self.conv1(x)))  # Apply initial convolution, ReLU, and pooling
        x = self.resnet_block1(x)  # Pass through first ResNetBlock
        x = self.resnet_block2(x)  # Pass through second ResNetBlock
        x = self.resnet_block3(x)  # Pass through third ResNetBlock
        x = self.pool(x)  # Pooling
        x = x.view(-1, 64 * 8 * 8)  # Reshape tensor
        x = torch.relu(self.fc1(x))  # Apply ReLU to first fully connected layer
        x = self.fc2(x)  # Apply second fully connected layer
        return x  # Return output

"""### Loading CIFAR10 training and test data
Obtaining better result by taking mean and std values on data loaded instead of using mean and std from cnn tutorial.
"""

import torchvision.transforms as transforms  # Import transforms module from torchvision for data transformations
from torchvision.datasets import CIFAR10  # Import CIFAR10 dataset from torchvision.datasets
from torch.utils.data import DataLoader  # Import DataLoader for batch loading of data
import matplotlib.pyplot as plt  # Import matplotlib.pyplot for plotting
import numpy as np  # Import numpy for numerical operations

## mean = [0.4914, 0.4822, 0.4465]
## std = [0.2470, 0.2435, 0.2616]
## The above mean values are given in CNN Tutorial

# Will give mean and standard deviation values by calculating based on loaded data (better performance)

# Define data augmentation and normalization transformations
transform_mean = transforms.Compose([
    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor
])


# Define CIFAR-10 train and test datasets with the specified transformations
trainset_mean = CIFAR10(root='./data', train=True, download=True, transform=transform_mean)
#testset_mean = CIFAR10(root='./data', train=False, download=True, transform=transform_mean)


# Compute mean and standard deviation for each channel
mean = torch.stack([t.mean(dim=(1, 2)) for t, _ in trainset_mean]).mean(dim=0)
std = torch.stack([t.std(dim=(1, 2)) for t, _ in trainset_mean]).mean(dim=0)

# Define data augmentation and normalization transformations
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor
    transforms.Normalize(mean, std)  # Normalize the image
])

# Define CIFAR-10 train and test datasets with the specified transformations
trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)
testset = CIFAR10(root='./data', train=False, download=True, transform=transform)


# Function to display sample images from the dataset
def show_samples(dataset, num_samples=5):
    classes = dataset.classes
    fig, axes = plt.subplots(1, num_samples, figsize=(12, 3))
    for i in range(num_samples):
        image, label = dataset[i]
        axes[i].imshow(np.transpose(image.numpy(), (1, 2, 0)))
        axes[i].set_title(classes[label])
        axes[i].axis('off')
    plt.show()

# Print the shape of the first image in the train set
print("Shape of the first image in train set:", trainset[0][0].shape)

# Print the shape of the first image in the test set
print("Shape of the first image in test set:", testset[0][0].shape)

# Display sample images from the train set
print("Sample images from the train set:")
show_samples(trainset)

# Define data loaders
trainloader = DataLoader(trainset, batch_size=256, shuffle=True)
testloader = DataLoader(testset, batch_size=256, shuffle=False)

"""### Initializing Models and printing model parameters
Loss Function chosen was CrossEntropyLoss and Optimizer chosen was Adam
"""

# Create instances of CNNVanilla and CNNResNet models
cnn_vanilla = CNNVanilla()
cnn_resnet = CNNResNet()

# Print the architectures of the models
print(cnn_vanilla)
print(cnn_resnet)

# Set device to GPU if available, else use CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
cnn_vanilla.to(device)  # Move CNNVanilla model to device
cnn_resnet.to(device)  # Move CNNResNet model to device

# Print the number of parameters in each model
print("Number of parameters in CNN-Vanilla:", sum(p.numel() for p in cnn_vanilla.parameters()))
print("Number of parameters in CNN-ResNet:", sum(p.numel() for p in cnn_resnet.parameters()))

# Define loss function and optimizers for each model
criterion = nn.CrossEntropyLoss()  # Cross-entropy loss function
optimizer_vanilla = optim.Adam(cnn_vanilla.parameters(), lr=0.001)  # Adam optimizer for CNNVanilla
optimizer_resnet = optim.Adam(cnn_resnet.parameters(), lr=0.001)  # Adam optimizer for CNNResNet

# Perform a forward pass through CNNResNet model with random input tensor
ans = cnn_resnet(torch.rand([256,3,32,32]).to(device))
print(ans.shape)  # Print the shape of the output tensor

"""### Defining function to train model"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Function to train the model and compute accuracy
def train_model(model, trainloader, criterion, optimizer, num_epochs=50):
    model.train()  # Set the model to training mode
    train_accuracy_history = []  # List to store training accuracy for each epoch
    for epoch in range(num_epochs):
        running_loss = 0.0  # Initialize running loss
        correct_predictions = 0  # Initialize number of correct predictions
        total_predictions = 0  # Initialize total number of predictions
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data[0].to(device), data[1].to(device)  # Get inputs and labels
            optimizer.zero_grad()  # Zero the parameter gradients
            outputs = model(inputs)  # Forward pass
            loss = criterion(outputs, labels)  # Compute the loss
            loss.backward()  # Backward pass
            optimizer.step()  # Optimize

            running_loss += loss.item()  # Accumulate loss
            _, predicted = torch.max(outputs.data, 1)  # Get predicted labels
            total_predictions += labels.size(0)  # Increment total predictions
            correct_predictions += (predicted == labels).sum().item()  # Increment correct predictions

        # Calculate accuracy for the epoch
        epoch_accuracy = correct_predictions / total_predictions
        train_accuracy_history.append(epoch_accuracy)  # Append accuracy to history

        # Print epoch statistics
        print(f'Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}, Accuracy: {epoch_accuracy:.2%}')

    return train_accuracy_history  # Return training accuracy history

"""### Training CNN-Vanilla"""

# Train CNN-Vanilla
print("Training CNN-Vanilla:")
train_accuracy_vanilla = train_model(cnn_vanilla, trainloader, criterion, optimizer_vanilla)

"""### Training CNN-Resnet"""

# Train CNN-ResNet
print("Training CNN-ResNet:")
train_accuracy_resnet = train_model(cnn_resnet, trainloader, criterion, optimizer_resnet)

"""# Experiment 1

### Plotting Training accuracy vs Epochs of CNN-Resnet and CNN-Vanilla
"""

# Plotting training accuracy vs. epochs
plt.plot(range(1, len(train_accuracy_vanilla) + 1), train_accuracy_vanilla, label='CNN-Vanilla')
plt.plot(range(1, len(train_accuracy_resnet) + 1), train_accuracy_resnet, label='CNN-ResNet')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Training Accuracy vs. Epochs')
plt.legend()
plt.grid(True)
plt.show()

"""### Evaluating Model Accuracy on Test Data"""

# Define function to evaluate model
def evaluate_model(model, testloader):
    model.eval()  # Set the model to evaluation mode
    correct = 0  # Initialize number of correct predictions
    total = 0  # Initialize total number of predictions
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)  # Get images and labels
            outputs = model(images)  # Forward pass
            _, predicted = torch.max(outputs, 1)  # Get predicted labels
            total += labels.size(0)  # Increment total predictions
            correct += (predicted == labels).sum().item()  # Increment correct predictions

    accuracy = 100 * correct / total  # Compute accuracy
    return accuracy  # Return accuracy

# Evaluate CNN-Vanilla
accuracy_vanilla = evaluate_model(cnn_vanilla, testloader)
print(f'Accuracy of CNN-Vanilla on the test set: {accuracy_vanilla:.2f}%')

# Evaluate CNN-ResNet
accuracy_resnet = evaluate_model(cnn_resnet, testloader)
print(f'Accuracy of CNN-ResNet on the test set: {accuracy_resnet:.2f}%')

"""### Comparing number of Parameters and Performance"""

# Comparing number of parameters and performances
print("Number of parameters in CNN-Vanilla:", sum(p.numel() for p in cnn_vanilla.parameters()))
print("Number of parameters in CNN-ResNet:", sum(p.numel() for p in cnn_resnet.parameters()))

print(f'Accuracy of CNN-Vanilla on the test set: {accuracy_vanilla:.2f}%')
print(f'Accuracy of CNN-ResNet on the test set: {accuracy_resnet:.2f}%')

# Compare accuracies of CNN-Vanilla and CNN-ResNet
if accuracy_vanilla > accuracy_resnet:
    print("CNN-Vanilla performs better on the given problem.")
elif accuracy_vanilla < accuracy_resnet:
    print("CNN-ResNet performs better on the given problem.")
else:
    print("Both CNN-Vanilla and CNN-ResNet perform equally well on the given problem.")

"""## CNN-Resnet is the Winner !!
We will use CNN-Resnet for further analysis from here on

# Experiment 2 : Study the Effect of Data Normalization

### Loading Data without Normalization
"""

# Define normalization transformations
transform_without_norm = transforms.Compose([
    transforms.ToTensor() , # Convert PIL image to PyTorch tensor
])

# Define CIFAR-10 train and test datasets with the specified transformations
trainset_without_norm = CIFAR10(root='./data', train=True, download=True, transform=transform_without_norm )
testset_without_norm = CIFAR10(root='./data', train=False, download=True, transform=transform_without_norm )

# Display sample images from the train set
print("Sample images from the train set:")
show_samples(trainset_without_norm)

"""### Initializing and Training unnormalized model"""

# Create an instance of CNNResNet model without normalization
cnn_resnet_without_norm = CNNResNet()
cnn_resnet_without_norm.to(device)  # Move model to device

# Define optimizer for CNN-ResNet without normalization
optimizer_resnet_without_norm = optim.Adam(cnn_resnet_without_norm.parameters(), lr=0.001)

# Define data loaders for CNN-ResNet without normalization
trainloader_without_norm = DataLoader(trainset_without_norm, batch_size=256, shuffle=True)
testloader_without_norm = DataLoader(testset_without_norm, batch_size=256, shuffle=False)

# Train CNN-ResNet without data normalization
train_accuracy_resnet_unnormalized = train_model(cnn_resnet_without_norm, trainloader_without_norm, criterion, optimizer_resnet_without_norm)

"""### Plotting Training accuracy vs Epochs for CNN-Resnet with and without normalization"""

# Plotting training accuracy vs. epochs for both cases
plt.plot(range(1, len(train_accuracy_resnet) + 1), train_accuracy_resnet, label='With Normalization')
plt.plot(range(1, len(train_accuracy_resnet_unnormalized) + 1), train_accuracy_resnet_unnormalized, label='Without Normalization')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Training Accuracy vs. Epochs (CNN-ResNet)')
plt.legend()
plt.grid(True)
plt.show()

"""### Evaluating CNN-Resent accuracy on test data with and without Data Normalization"""

# Evaluate CNN-ResNet with data normalization
accuracy_resnet_normalized = evaluate_model(cnn_resnet, testloader)
print(f'Accuracy of CNN-ResNet with data normalization on the test set: {accuracy_resnet_normalized:.2f}%')

# Evaluate CNN-ResNet without data normalization
accuracy_resnet_unnormalized = evaluate_model(cnn_resnet_without_norm, testloader_without_norm)
print(f'Accuracy of CNN-ResNet without data normalization on the test set: {accuracy_resnet_unnormalized:.2f}%')

"""## We are getting better accuracy with Data Normalization !!

# Experiment 3: Study the Effect of Different Optimizers
Best network : CNN-Resnet &
Best choice of Data Normalization : With Normalization

### Defining models and its optimizers
"""

# Define models and optimizers
cnn_resnet_sgd = CNNResNet().to(device)
cnn_resnet_mb_no_momentum = CNNResNet().to(device)
cnn_resnet_mb_momentum = CNNResNet().to(device)
cnn_resnet_adam = CNNResNet().to(device)

optimizer_resnet_sgd = optim.SGD(cnn_resnet_sgd.parameters(), lr=0.001)
optimizer_resnet_mb_no_momentum = optim.SGD(cnn_resnet_mb_no_momentum.parameters(), lr=0.001, momentum=0)
optimizer_resnet_mb_momentum = optim.SGD(cnn_resnet_mb_momentum.parameters(), lr=0.001, momentum=0.9)
optimizer_resnet_adam = optim.Adam(cnn_resnet_adam.parameters(), lr=0.001)

# Train the models with different optimizers
optimizers = {
    'SGD': optimizer_resnet_sgd,
    'Mini-batch (No Momentum)': optimizer_resnet_mb_no_momentum,
    'Mini-batch (Momentum 0.9)': optimizer_resnet_mb_momentum,
    'Adam': optimizer_resnet_adam
}

"""### Training Models"""

train_accuracy_histories = {}  # Dictionary to store training accuracy histories
for optimizer_name, optimizer in optimizers.items():  # Iterate over optimizers
    print(f"Training with {optimizer_name}...")
    # Determine the CNN-ResNet model based on optimizer_name
    if optimizer_name == 'Mini-batch (No Momentum)':
        cnn_resnet_model = cnn_resnet_mb_no_momentum
    elif optimizer_name == 'Mini-batch (Momentum 0.9)':
        cnn_resnet_model = cnn_resnet_mb_momentum
    else:
        cnn_resnet_model = locals()[f'cnn_resnet_{optimizer_name.lower().replace(" ", "_")}']
    # Train the model and store the training accuracy history
    train_accuracy_histories[optimizer_name] = train_model(cnn_resnet_model, trainloader, criterion, optimizer)

"""### Evaluating Models' Accuracies"""

# Evaluate models
eval_results = {}
for optimizer_name, model in zip(optimizers.keys(), [cnn_resnet_sgd, cnn_resnet_mb_no_momentum, cnn_resnet_mb_momentum, cnn_resnet_adam]):
    accuracy = evaluate_model(model, testloader)
    eval_results[optimizer_name] = accuracy
    print(f"Accuracy of model trained with {optimizer_name} optimizer: {accuracy:.2f}%")

"""### Plotting Evaluation Results"""

import matplotlib.pyplot as plt
# Plotting evaluation results
plt.figure(figsize=(10, 6))
plt.bar(eval_results.keys(), eval_results.values())
plt.xlabel('Optimizers')
plt.ylabel('Test Accuracy')
plt.title('Effect of Different Optimizers on Test Accuracy')
plt.ylim(0, 100)
plt.grid(axis='y')
plt.show()

"""### Plotting Training Accuracy vs Epochs for each Optimizer"""

# Plotting training accuracy vs epochs for each optimizer
plt.figure(figsize=(12, 8))
for optimizer_name, history in train_accuracy_histories.items():
    plt.plot(range(1, len(history) + 1), history, label=optimizer_name)

plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Training Accuracy vs. Epochs for Different Optimizers')
plt.legend()
plt.grid(True)
plt.show()

"""### Determining the Best Optimizer"""

# Determine the best optimizer
best_optimizer = max(eval_results, key=eval_results.get)
print(f"The best optimizer is: {best_optimizer} with a test accuracy of {eval_results[best_optimizer]:.2f}%")

"""## Adam is the Best Optimizer !!

# Experiment 4: Study the Effect of Network Depth

### Defining CNN network with 4 resnet blocks and 2 FC Layers
"""

class CNNResNet4Level(nn.Module):
    def __init__(self):
        super(CNNResNet4Level, self).__init__()  # Initialize superclass
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # First convolutional layer
        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer
        # Define four ResNet blocks
        self.resnet_block1 = ResNetBlock(32, 64)
        self.resnet_block2 = ResNetBlock(64, 64)
        self.resnet_block3 = ResNetBlock(64, 64)
        self.resnet_block4 = ResNetBlock(64, 64)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # First fully connected layer
        self.fc2 = nn.Linear(128, 10)  # Second fully connected layer (output layer)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))  # Convolutional layer followed by ReLU and pooling
        # Pass through each ResNet block
        x = self.resnet_block1(x)
        x = self.resnet_block2(x)
        x = self.resnet_block3(x)
        x = self.resnet_block4(x)
        x = self.pool(x)  # Pooling layer
        x = x.view(-1, 64 * 8 * 8)  # Flatten the output
        x = torch.relu(self.fc1(x))  # Fully connected layer followed by ReLU
        x = self.fc2(x)  # Output layer
        return x

"""## Defining CNN Network with 3 Resnet Blocks and 4 Fully Connected Layers"""

class CNNResNet3Level(nn.Module):
    def __init__(self):
        super(CNNResNet3Level, self).__init__()  # Initialize superclass
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # First convolutional layer
        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer
        # Define three ResNet blocks
        self.resnet_block1 = ResNetBlock(32, 64)
        self.resnet_block2 = ResNetBlock(64, 64)
        self.resnet_block3 = ResNetBlock(64, 64)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # First fully connected layer
        self.fc2 = nn.Linear(128, 64)  # Second fully connected layer
        self.fc3 = nn.Linear(64, 64)  # Third fully connected layer
        self.fc4 = nn.Linear(64, 10)  # Output layer

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))  # Convolutional layer followed by ReLU and pooling
        # Pass through each ResNet block
        x = self.resnet_block1(x)
        x = self.resnet_block2(x)
        x = self.resnet_block3(x)
        x = self.pool(x)  # Pooling layer
        x = x.view(-1, 64 * 8 * 8)  # Flatten the output
        x = torch.relu(self.fc1(x))  # First fully connected layer followed by ReLU
        x = torch.relu(self.fc2(x))  # Second fully connected layer followed by ReLU
        x = torch.relu(self.fc3(x))  # Third fully connected layer followed by ReLU
        x = self.fc4(x)  # Output layer
        return x

"""## There is an effective increase of 2 layers on top of our original CNN-Resnet Model in the above 2 Models that have been defined
Either 2 FCs added or a Resnet block that has 2 Convolutional Layers were added

## Initializing both the models and defining optimizers
"""

# Initialize models
cnn_resnet_4level = CNNResNet4Level().to(device)  # CNNResNet4Level model moved to device
cnn_resnet_3level = CNNResNet3Level().to(device)  # CNNResNet3Level model moved to device

# Define optimizers
optimizer_resnet_4level = optim.Adam(cnn_resnet_4level.parameters(), lr=0.001)  # Adam optimizer for CNNResNet4Level
optimizer_resnet_3level = optim.Adam(cnn_resnet_3level.parameters(), lr=0.001)  # Adam optimizer for CNNResNet3Level

print(cnn_resnet_4level)  # Print CNNResNet4Level model
print(cnn_resnet_3level)  # Print CNNResNet3Level model

"""### Training CNN Resnet with 4 Resnet blocks and 2 Fcs"""

# Train models
print("Training Resnet 4 Level")
train_accuracy_resnet_4level = train_model(cnn_resnet_4level, trainloader, criterion, optimizer_resnet_4level)

"""### Training CNN Resnet with 3 Resnet blocks and 4 Fcs"""

print("Training Resnet 3 Level")
train_accuracy_resnet_3level = train_model(cnn_resnet_3level, trainloader, criterion, optimizer_resnet_3level)

"""### Evaluating Models"""

# Evaluate the models
accuracy_resnet_4level = evaluate_model(cnn_resnet_4level, testloader)
accuracy_resnet_3level = evaluate_model(cnn_resnet_3level, testloader)

# Print results
print("Accuracy of CNN-ResNet with 4-level blocks and 2 FC on the test set:", accuracy_resnet_4level)
print("Accuracy of CNN-ResNet with 3-level blocks and 4 FC on the test set:", accuracy_resnet_3level)
print("Accuracy of CNN-ResNet with 3-level blocks and 2 FC on the test set:", accuracy_resnet)

"""### Plotting Training Accuracy vs Epochs for each model"""

# Plotting training accuracy vs. epochs for each model
plt.plot(range(1, len(train_accuracy_resnet_4level) + 1), train_accuracy_resnet_4level, label='4-level Blocks')
plt.plot(range(1, len(train_accuracy_resnet_3level) + 1), train_accuracy_resnet_3level, label='3-level Blocks')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Training Accuracy vs. Epochs for Different Network Depths')
plt.legend()
plt.grid(True)
plt.show()

"""### Comparing number of parameters in the 3 Resnet Models"""

# Commenting on the performance and number of parameters
print("Number of parameters in CNN-ResNet with 4-level blocks and 2 FC:", sum(p.numel() for p in cnn_resnet_4level.parameters()))
print("Number of parameters in CNN-ResNet with 3-level blocks and 4 FC:", sum(p.numel() for p in cnn_resnet_3level.parameters()))
print("Number of parameters in CNN-ResNet with 3-level blocks and 2 FC:", sum(p.numel() for p in cnn_resnet.parameters()))

"""### Comparing Resnet model with 3 blocks and 4 Fcs & Resnet model with 4 blocks and 2 Fcs"""

if accuracy_resnet_4level > accuracy_resnet_3level:
    print("CNN-ResNet with 4-level blocks performs better on the given problem.")
elif accuracy_resnet_4level < accuracy_resnet_3level:
    print("CNN-ResNet with 3-level blocks performs better on the given problem.")
else:
    print("Both CNN-ResNet models perform equally well on the given problem.")

"""## CNN-Resnet with 4 blocks and 2 Fcs is the Winner amongst the 2 !!

### Comparing Resnet model with 3 blocks and 2 Fcs & Resnet model with 3 blocks and 4 Fcs & Resnet model with 4 blocks and 2 Fcs
"""

if accuracy_resnet_4level > accuracy_resnet_3level:
    if accuracy_resnet_4level > accuracy_resnet:
        print("Among the 3 Models, CNN-ResNet with 4-level blocks and 2 Fcs performs better on the given problem.")
    else:
        print("Among the 3 Models, Original CNN-ResNet performs better on the given problem.")
elif accuracy_resnet_4level < accuracy_resnet_3level:
    if accuracy_resnet_3level > accuracy_resnet:
        print("Among the 3 Models, CNN-ResNet with 3-level blocks and 4 Fcs performs better on the given problem.")
    else:
        print("Among the 3 Models, Original CNN-ResNet performs better on the given problem.")
else:
    if accuracy_resnet_3level > accuracy_resnet:
        print("Among the 3 Models, CNN-ResNet with 3-level blocks & 4 Fcs performs equally well as CNN-ResNet with 4-level blocks & 2 Fcs on the given problem.")
    else:
        print("Among the 3 Models, Original CNN-ResNet model perform equally well on the given problem.")

"""## CNN-Resnet with 4 blocks and 2 Fcs is the Winner amongst the 2 !! Therefore increasing conv layers is better

# Experiment 5: Study the Effect of Different Regularizers

### Defining ResNet block with BatchNorm
"""

# Define the ResNet block with BatchNorm
class ResNetBlock_BatchNorm(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResNetBlock_BatchNorm, self).__init__()
        # Define convolutional layers
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        # Define batch normalization layers
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)
        # Define activation function
        self.relu = nn.ReLU(inplace=True)
        # Define shortcut connection
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Sequential()

    def forward(self, x):
        # Forward pass through the block
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(residual)
        out = self.relu(out)
        return out

# Define CNNResNet with BatchNorm
class CNNResNet_BatchNorm(nn.Module):
    def __init__(self):
        super(CNNResNet_BatchNorm, self).__init__()
        # Define initial convolutional layer and pooling layer
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        # Define ResNet blocks
        self.resnet_block1 = ResNetBlock(32, 64)
        self.resnet_block2 = ResNetBlock(32, 64)
        self.resnet_block3 = ResNetBlock(32, 64)
        # Define fully connected layers
        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # Updated fc1 input size
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # Forward pass through the network
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.resnet_block1(x)
        x = self.pool(x)
        x = x.view(-1, 64 * 8 * 8)  # Adjusted view size
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

"""### Defining ResNet block with Dropout"""

# Define the ResNet block with Dropout
class ResNetBlock_Dropout(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResNetBlock_Dropout, self).__init__()
        # Define convolutional layers
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        # Define dropout layers
        self.dropout = nn.Dropout(0.5)
        # Define activation function
        self.relu = nn.ReLU(inplace=True)
        # Define shortcut connection
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.Dropout(0.5)
            )
        else:
            self.shortcut = nn.Sequential()

    def forward(self, x):
        # Forward pass through the block
        residual = x
        out = self.relu(self.dropout(self.conv1(x)))
        out = self.dropout(self.conv2(out))
        out += self.shortcut(residual)
        out = self.relu(out)
        return out

# Define CNNResNet with Dropout
class CNNResNet_Dropout(nn.Module):
    def __init__(self):
        super(CNNResNet_Dropout, self).__init__()
        # Define initial convolutional layer and pooling layer
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        # Define ResNet blocks
        self.resnet_block1 = ResNetBlock_Dropout(32, 64)
        self.resnet_block2 = ResNetBlock_Dropout(32, 64)
        self.resnet_block3 = ResNetBlock_Dropout(32, 64)
        # Define fully connected layers
        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # Updated fc1 input size
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # Forward pass through the network
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.resnet_block1(x)
        x = self.pool(x)
        x = x.view(-1, 64 * 8 * 8)  # Adjusted view size - Flattened
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

"""### Defining ResNet block with BatchNorm and Dropout"""

# Define the ResNet block with Batchnorm and Dropout
class ResNetBlock_Dropout_BatchNorm(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResNetBlock_Dropout_BatchNorm, self).__init__()
        # Define convolutional layers
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        # Define batch normalization layers
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)
        # Define dropout layer
        self.dropout = nn.Dropout(0.5)
        # Define activation function
        self.relu = nn.ReLU(inplace=True)
        # Define shortcut connection
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.Dropout(0.5),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Sequential()

    def forward(self, x):
        # Forward pass through the block
        residual = x
        out = self.relu(self.bn1(self.dropout(self.conv1(x))))
        out = self.bn2(self.dropout(self.conv2(out)))
        out += self.shortcut(residual)
        out = self.relu(out)
        return out

# Define CNNResNet with Batchnorm and Dropout
class CNNResNet_Dropout_BatchNorm(nn.Module):
    def __init__(self):
        super(CNNResNet_Dropout_BatchNorm, self).__init__()
        # Define initial convolutional layer and pooling layer
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        # Define ResNet blocks
        self.resnet_block1 = ResNetBlock_Dropout_BatchNorm(32, 64)
        self.resnet_block2 = ResNetBlock_Dropout_BatchNorm(32, 64)
        self.resnet_block3 = ResNetBlock_Dropout_BatchNorm(32, 64)
        # Define fully connected layers
        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # Updated fc1 input size
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # Forward pass through the network
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.resnet_block1(x)
        x = self.pool(x)
        x = x.view(-1, 64 * 8 * 8)  # Adjusted view size
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

"""### Initializing Models and their Optimizers"""

# Initialize models
cnn_resnet_batchnorm = CNNResNet_BatchNorm()
cnn_resnet_dropout = CNNResNet_Dropout()
cnn_resnet_dropout_batchnorm = CNNResNet_Dropout_BatchNorm()

cnn_resnet_batchnorm.to(device)
cnn_resnet_dropout.to(device)
cnn_resnet_dropout_batchnorm.to(device)

# Define optimizers
optimizer_cnn_resnet_batchnorm = optim.Adam(cnn_resnet_batchnorm.parameters(), lr=0.001)
optimizer_cnn_resnet_dropout = optim.Adam(cnn_resnet_dropout.parameters(), lr=0.001)
optimizer_cnn_resnet_dropout_batchnorm = optim.Adam(cnn_resnet_dropout_batchnorm.parameters(), lr=0.001)

print(cnn_resnet_batchnorm)
print(cnn_resnet_dropout)
print(cnn_resnet_dropout_batchnorm)

"""### Defining Training Function which also evaluates model on test data every Epoch"""

def train_test_model(model, trainloader, criterion, optimizer, testloader, num_epochs=50):
    model.train()
    train_accuracy_history = []  # List to store training accuracy for each epoch
    test_accuracy_history = []   # List to store test accuracy for each epoch

    for epoch in range(num_epochs):
        running_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data[0].to(device), data[1].to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_predictions += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()

        # Calculate accuracy for the epoch
        epoch_accuracy = correct_predictions / total_predictions
        train_accuracy_history.append(epoch_accuracy)

        # Evaluation phase
        test_accuracy = evaluate_model(model, testloader)
        test_accuracy_history.append(test_accuracy)

        print(f'Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}, Accuracy: {epoch_accuracy:.2%}')

    return test_accuracy_history

"""### Training Model with  Batch Normalization"""

# Train the models with different regularizers
print("Training CNN Resnet with Batch Norm")
test_accuracy_bn = train_test_model(cnn_resnet_batchnorm, trainloader, criterion, optimizer_cnn_resnet_batchnorm, testloader)

"""### Training Model with Dropout"""

print("Training CNN Resnet with Dropout")
test_accuracy_dropout = train_test_model(cnn_resnet_dropout, trainloader, criterion, optimizer_cnn_resnet_dropout, testloader)

"""### Training Model with  Batch Normalization and Dropout"""

print("Training CNN Resnet with Batch Norm and Dropout")
test_accuracy_dropout_bn = train_test_model(cnn_resnet_dropout_batchnorm, trainloader, criterion, optimizer_cnn_resnet_dropout_batchnorm, testloader)

"""### Plot testing accuracy vs epochs for the three cases"""

# Plot testing accuracy vs epochs for the three cases
plt.plot(range(1, len(test_accuracy_bn) + 1), test_accuracy_bn, label='Batch Normalization')
plt.plot(range(1, len(test_accuracy_dropout) + 1), test_accuracy_dropout, label='Dropout')
plt.plot(range(1, len(test_accuracy_dropout_bn) + 1), test_accuracy_dropout_bn, label='Batch Norm + Dropout')
plt.title('Testing Accuracy vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Testing Accuracy')
plt.legend()
plt.show()

"""### Comparing Performance and determining Best choice"""

# Compare the performance and determine the best choice
print(f"Accuracy of BatchNorm is: {test_accuracy_bn[-1]}")
print(f"Accuracy of Dropout is: {test_accuracy_dropout[-1]}")
print(f"Accuracy of BatchNorm and Dropout is: {test_accuracy_dropout_bn[-1]}")
best_accuracy = max(test_accuracy_bn[-1], test_accuracy_dropout[-1], test_accuracy_dropout_bn[-1])
best_choice = None
if best_accuracy == test_accuracy_bn[-1]:
    best_choice = 'Batch Normalization'
elif best_accuracy == test_accuracy_dropout[-1]:
    best_choice = 'Dropout'
else:
    best_choice = 'Batch Norm + Dropout'

print(f"The best choice among the three regularizers is: {best_choice}")